{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/PhamVuHuyenTrang/Hanoi-Real-Estate-Analysis-and-Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SP500\n",
    "Russel 2000 Index\n",
    "Dollar Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***1. Phân tích crawl from my teammate.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dựa trên nội dung của sổ tay Jupyter đầu tiên `crawl_gold_price.ipynb`, tôi sẽ cung cấp một tóm tắt và phân tích chi tiết về cách thức hoạt động của nó:\n",
    "\n",
    "### Tóm Tắt\n",
    "Sổ tay này chứa mã để thu thập dữ liệu giá vàng. Các bước chính bao gồm:\n",
    "\n",
    "1. **Cài Đặt Thư Viện:**\n",
    "   - Cài đặt `cloudscraper`, một thư viện Python dùng để vượt qua các biện pháp bảo vệ trang web như Cloudflare.\n",
    "\n",
    "2. **Thu Thập Dữ liệu:**\n",
    "   - Có vẻ như sổ tay thực hiện việc thu thập dữ liệu giá vàng từ một nguồn không rõ ràng trong nội dung đã hiển thị.\n",
    "\n",
    "3. **Xử Lý Dữ Liệu:**\n",
    "   - Dữ liệu được xử lý và chuyển đổi thành định dạng DataFrame của Pandas. Có một phần mã kiểm tra kiểu dữ liệu của các cột trong DataFrame.\n",
    "\n",
    "4. **Lưu Trữ Dữ Liệu:**\n",
    "   - Cuối cùng, dữ liệu được lưu vào một tệp CSV, có lẽ cho mục đích phân tích sau này.\n",
    "\n",
    "### Chi Tiết Code\n",
    "1. **Cài Đặt Thư Viện:**\n",
    "   ```python\n",
    "   !pip install cloudscraper\n",
    "   ```\n",
    "   Lệnh này cài đặt `cloudscraper` thông qua pip.\n",
    "\n",
    "2. **Thu Thập Dữ liệu:**\n",
    "   - Phần này không hiển thị trong đoạn trích nội dung, nhưng có khả năng có sử dụng `cloudscraper` để truy cập và thu thập dữ liệu từ một trang web.\n",
    "\n",
    "3. **Xử Lý Dữ Liệu:**\n",
    "   - Một biến DataFrame được gọi là `df`, có lẽ chứa dữ liệu giá vàng.\n",
    "   - Các cột dữ liệu bao gồm ngày, giá vàng, giá mở cửa, giá cao nhất và giá thấp nhất.\n",
    "   - Kiểm tra kiểu dữ liệu của các cột:\n",
    "     ```python\n",
    "     df.dtypes\n",
    "     ```\n",
    "\n",
    "4. **Lưu Trữ Dữ Liệu:**\n",
    "   ```python\n",
    "   df.to_csv('gold.csv', index=False)\n",
    "   print(\"Done!!\")\n",
    "   ```\n",
    "   Mã này lưu DataFrame vào một tệp CSV và in ra thông báo \"Done!!\" khi hoàn thành.\n",
    "\n",
    "Để cung cấp thông tin chi tiết hơn về hai sổ tay còn lại, tôi sẽ tiếp tục phân tích chúng.\n",
    "\n",
    "Dựa trên nội dung của sổ tay Jupyter thứ hai `crawl_MSCI_EM_EFT.ipynb`, đây là tóm tắt và phân tích chi tiết:\n",
    "\n",
    "### Tóm Tắt\n",
    "Sổ tay này dường như dùng để thu thập và xử lý dữ liệu về chỉ số ETF MSCI EM (Emerging Markets). Các bước chính bao gồm:\n",
    "\n",
    "1. **Cài Đặt Thư Viện:**\n",
    "   - Cài đặt `cloudscraper`, tương tự như sổ tay đầu tiên.\n",
    "\n",
    "2. **Thu Thập và Xử Lý Dữ liệu:**\n",
    "   - Đọc dữ liệu từ một tệp HTML lưu trữ trên máy tính cá nhân của người dùng.\n",
    "   - Sử dụng `BeautifulSoup` từ thư viện `bs4` để phân tích cú pháp HTML và trích xuất dữ liệu.\n",
    "   - Khởi tạo và điền dữ liệu vào các danh sách cho ngày, giá và các thông số khác.\n",
    "\n",
    "3. **Tạo DataFrame và Lưu Trữ Dữ liệu:**\n",
    "   - Chuyển đổi dữ liệu thành DataFrame của Pandas.\n",
    "   - Lưu DataFrame vào tệp CSV.\n",
    "\n",
    "### Chi Tiết Code\n",
    "1. **Cài Đặt Thư Viện:**\n",
    "   ```python\n",
    "   !pip install cloudscraper\n",
    "   ```\n",
    "   Cài đặt `cloudscraper` qua pip.\n",
    "\n",
    "2. **Thu Thập và Xử Lý Dữ liệu:**\n",
    "   - Import các thư viện cần thiết như `pandas`, `BeautifulSoup`, `re`.\n",
    "   - Đọc dữ liệu từ tệp HTML:\n",
    "     ```python\n",
    "     with open(\"/path/to/EFT.txt\", \"r\", encoding='utf-8') as f:\n",
    "         html = f.read()\n",
    "     soup = BeautifulSoup(html, 'html.parser')\n",
    "     ```\n",
    "   - Trích xuất dữ liệu từ HTML và lưu vào các danh sách.\n",
    "\n",
    "3. **Tạo DataFrame và Lưu Trữ Dữ liệu:**\n",
    "   - Tạo DataFrame từ các danh sách:\n",
    "     ```python\n",
    "     df = pd.DataFrame({\n",
    "         'date': dates,\n",
    "         'MSCI_EM_EFT': prices,\n",
    "         # possibly other columns...\n",
    "     })\n",
    "     ```\n",
    "   - Lưu DataFrame vào tệp CSV:\n",
    "     ```python\n",
    "     df.to_csv('MSCI_EM_EFT.csv', index=False)\n",
    "     print(\"Done!!\")\n",
    "     ```\n",
    "\n",
    "Giờ tôi sẽ tiếp tục phân tích sổ tay Jupyter thứ ba để cung cấp thông tin chi tiết về nó.\n",
    "\n",
    "Dựa trên nội dung của sổ tay Jupyter thứ ba `crawl_NLR_EFT.ipynb`, dưới đây là tóm tắt và phân tích chi tiết:\n",
    "\n",
    "### Tóm Tắt\n",
    "Sổ tay này được sử dụng để thu thập và xử lý dữ liệu về chỉ số ETF NLR (Nuclear Energy ETF). Các bước chính bao gồm:\n",
    "\n",
    "1. **Cài Đặt Thư Viện:**\n",
    "   - Cài đặt `cloudscraper`, giống như hai sổ tay trước.\n",
    "\n",
    "2. **Thu Thập và Xử Lý Dữ liệu:**\n",
    "   - Đọc dữ liệu từ một tệp HTML.\n",
    "   - Sử dụng `BeautifulSoup` để phân tích cú pháp HTML và trích xuất dữ liệu.\n",
    "   - Lưu trữ dữ liệu vào các danh sách cho ngày, giá và các thông số khác.\n",
    "\n",
    "3. **Tạo DataFrame và Lưu Trữ Dữ liệu:**\n",
    "   - Chuyển đổi dữ liệu thành DataFrame của Pandas.\n",
    "   - Lưu DataFrame vào tệp CSV.\n",
    "\n",
    "### Chi Tiết Code\n",
    "1. **Cài Đặt Thư Viện:**\n",
    "   ```python\n",
    "   !pip install cloudscraper\n",
    "   ```\n",
    "   Cài đặt `cloudscraper` qua pip.\n",
    "\n",
    "2. **Thu Thập và Xử Lý Dữ liệu:**\n",
    "   - Import các thư viện cần thiết như `pandas`, `BeautifulSoup`, `re`.\n",
    "   - Đọc dữ liệu từ tệp HTML:\n",
    "     ```python\n",
    "     with open(\"/path/to/NLR_ETF.txt\", \"r\", encoding='utf-8') as f:\n",
    "         html = f.read()\n",
    "     soup = BeautifulSoup(html, 'html.parser')\n",
    "     ```\n",
    "   - Trích xuất dữ liệu từ HTML và lưu vào các danh sách.\n",
    "\n",
    "3. **Tạo DataFrame và Lưu Trữ Dữ liệu:**\n",
    "   - Tạo DataFrame từ các danh sách:\n",
    "     ```python\n",
    "     df = pd.DataFrame({\n",
    "         'date': dates,\n",
    "         'NLR_EFT': prices,\n",
    "         # possibly other columns...\n",
    "     })\n",
    "     ```\n",
    "   - Lưu DataFrame vào tệp CSV:\n",
    "     ```python\n",
    "     df.to_csv('NLR_EFT.csv', index=False)\n",
    "     print(\"Done!!\")\n",
    "     ```\n",
    "\n",
    "Như vậy, cả ba sổ tay Jupyter đều thực hiện việc thu thập dữ liệu từ các nguồn khác nhau (có thể là các trang web) và xử lý chúng để lưu vào các tệp CSV. Các bước bao gồm cài đặt thư viện cần thiết, phân tích dữ liệu HTML, xử lý và chuyển đổi dữ liệu thành định dạng DataFrame, và cuối cùng là lưu trữ dữ liệu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dựa trên đoạn trích đầu tiên của tệp script Python `crawl_silver_platinum_copper_MinhCuong.py`, dưới đây là phân tích tóm tắt và chi tiết:\n",
    "\n",
    "### Tóm Tắt\n",
    "Script này dường như được sử dụng để thu thập dữ liệu về giá của các kim loại như bạc, bạch kim và đồng. Các bước chính có thể bao gồm:\n",
    "\n",
    "1. **Sử Dụng Selenium:**\n",
    "   - Script sử dụng thư viện `selenium` để tương tác với trình duyệt web, một phương pháp phổ biến để thu thập dữ liệu từ các trang web có nội dung động.\n",
    "\n",
    "2. **Cài Đặt Trình Duyệt và Thu Thập Dữ liệu:**\n",
    "   - Khởi tạo trình duyệt Chrome và điều hướng đến URL cần thu thập dữ liệu.\n",
    "   - Có thể có các bước như chọn ngày và trích xuất dữ liệu từ trang web.\n",
    "\n",
    "3. **Xử Lý và Lưu Trữ Dữ liệu:**\n",
    "   - Chuyển đổi dữ liệu thu thập được thành định dạng DataFrame của Pandas.\n",
    "   - Có khả năng lưu trữ dữ liệu vào tệp CSV.\n",
    "\n",
    "### Chi Tiết Code\n",
    "1. **Import Thư Viện:**\n",
    "   - `selenium` và các mô-đun liên quan để tương tác với trình duyệt web.\n",
    "   - `pandas` và các thư viện hỗ trợ xử lý dữ liệu và thời gian.\n",
    "\n",
    "2. **Thu Thập Dữ liệu:**\n",
    "   - Khởi tạo webdriver của Chrome và điều hướng đến URL cụ thể.\n",
    "   - Sử dụng các hàm của Selenium để tìm kiếm và tương tác với các phần tử trên trang web.\n",
    "\n",
    "3. **Xử Lý Dữ liệu:**\n",
    "   - Trích xuất và xử lý dữ liệu, có thể bao gồm việc sử dụng `pandas` để tạo DataFrame.\n",
    "\n",
    "Để cung cấp thông tin chi tiết về hai tệp Jupyter notebook còn lại, tôi sẽ tiếp tục phân tích chúng.\n",
    "\n",
    "Dựa trên nội dung của sổ tay Jupyter `crude_oil_Long.ipynb`, đây là tóm tắt và phân tích chi tiết:\n",
    "\n",
    "### Tóm Tắt\n",
    "Sổ tay này chứa mã để thu thập dữ liệu về giá dầu thô. Các bước chính có thể bao gồm:\n",
    "\n",
    "1. **Sử Dụng Selenium:**\n",
    "   - Script sử dụng thư viện `selenium` để tương tác với trang web Yahoo Finance, nơi lưu trữ dữ liệu lịch sử giá dầu thô.\n",
    "\n",
    "2. **Cài Đặt Trình Duyệt và Thu Thập Dữ liệu:**\n",
    "   - Khởi tạo trình duyệt Edge và điều hướng đến URL cụ thể của Yahoo Finance.\n",
    "   - Có các bước để thiết lập ngày bắt đầu và kết thúc, sau đó trích xuất dữ liệu từ trang web.\n",
    "\n",
    "3. **Tạo DataFrame và Lưu Trữ Dữ liệu:**\n",
    "   - Chuyển đổi dữ liệu thu thập được thành DataFrame của Pandas.\n",
    "   - Có khả năng lưu trữ dữ liệu vào tệp CSV.\n",
    "\n",
    "### Chi Tiết Code\n",
    "1. **Import Thư Viện:**\n",
    "   - `selenium` để tương tác với trình duyệt web.\n",
    "   - `pandas` để xử lý và lưu trữ dữ liệu.\n",
    "   - Các thư viện khác hỗ trợ xử lý thời gian và dữ liệu.\n",
    "\n",
    "2. **Thu Thập Dữ liệu:**\n",
    "   - Khởi tạo trình duyệt Edge và điều hướng đến trang Yahoo Finance.\n",
    "   - Sử dụng Selenium để thiết lập ngày và trích xuất dữ liệu.\n",
    "\n",
    "3. **Xử Lý Dữ liệu:**\n",
    "   - Chuyển đổi dữ liệu thu thập được thành DataFrame.\n",
    "\n",
    "Giờ tôi sẽ tiếp tục phân tích sổ tay Jupyter thứ ba để cung cấp thông tin chi tiết.\n",
    "\n",
    "Dựa trên nội dung của sổ tay Jupyter `rough_rice_Long.ipynb`, đây là tóm tắt và phân tích chi tiết:\n",
    "\n",
    "### Tóm Tắt\n",
    "Sổ tay này chứa mã để thu thập dữ liệu về giá gạo thô. Các bước chính có thể bao gồm:\n",
    "\n",
    "1. **Sử Dụng Selenium:**\n",
    "   - Script sử dụng thư viện `selenium` để tương tác với trang web Yahoo Finance, nơi lưu trữ dữ liệu lịch sử giá gạo thô.\n",
    "\n",
    "2. **Cài Đặt Trình Duyệt và Thu Thập Dữ liệu:**\n",
    "   - Khởi tạo trình duyệt Edge và điều hướng đến URL cụ thể của Yahoo Finance.\n",
    "   - Có các bước để thiết lập ngày bắt đầu và kết thúc, sau đó trích xuất dữ liệu từ trang web.\n",
    "\n",
    "3. **Tạo DataFrame và Lưu Trữ Dữ liệu:**\n",
    "   - Chuyển đổi dữ liệu thu thập được thành DataFrame của Pandas.\n",
    "   - Có khả năng lưu trữ dữ liệu vào tệp CSV.\n",
    "\n",
    "### Chi Tiết Code\n",
    "1. **Import Thư Viện:**\n",
    "   - `selenium` để tương tác với trình duyệt web.\n",
    "   - `pandas` để xử lý và lưu trữ dữ liệu.\n",
    "   - Các thư viện khác hỗ trợ xử lý thời gian và dữ liệu.\n",
    "\n",
    "2. **Thu Thập Dữ liệu:**\n",
    "   - Khởi tạo trình duyệt Edge và điều hướng đến trang Yahoo Finance.\n",
    "   - Sử dụng Selenium để thiết lập ngày và trích xuất dữ liệu.\n",
    "\n",
    "3. **Xử Lý Dữ liệu:**\n",
    "   - Chuyển đổi dữ liệu thu thập được thành DataFrame.\n",
    "\n",
    "Như vậy, tất cả các script và sổ tay Jupyter này đều thực hiện việc thu thập dữ liệu từ các nguồn web, xử lý và chuyển đổi dữ liệu thành DataFrame, và có khả năng lưu trữ dữ liệu vào các tệp CSV. Các công cụ chính được sử dụng bao gồm Selenium để tương tác với trang web và Pandas cho xử lý dữ liệu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So sánh các phương pháp thu thập dữ liệu trong các script và sổ tay Jupyter đã được phân tích:\n",
    "\n",
    "### 1. Sử Dụng Thư Viện Selenium:\n",
    "\n",
    "- **Các Script và Jupyter Notebooks (`crude_oil_Long.ipynb`, `rough_rice_Long.ipynb`, `crawl_silver_platinum_copper_MinhCuong.py`):**\n",
    "   - Sử dụng Selenium để tương tác với trang web có nội dung động, chủ yếu là Yahoo Finance.\n",
    "   - Cho phép tương tác với các phần tử trang web như nút bấm, trường nhập liệu.\n",
    "   - Có thể đối phó với các trang web cần tương tác người dùng như chọn ngày tháng, cuộn trang.\n",
    "   - Phức tạp hơn về mặt kỹ thuật, đòi hỏi hiểu biết về cấu trúc trang web và cách sử dụng Selenium.\n",
    "\n",
    "### 2. Phân Tích Cú Pháp HTML với BeautifulSoup:\n",
    "\n",
    "- **Các Jupyter Notebooks (`crawl_gold_price.ipynb`, `crawl_MSCI_EM_EFT.ipynb`, `crawl_NLR_EFT.ipynb`):**\n",
    "   - Sử dụng BeautifulSoup để phân tích cú pháp HTML từ trang web hoặc tệp HTML lưu trữ.\n",
    "   - Thích hợp cho việc thu thập dữ liệu từ các trang web có nội dung tĩnh.\n",
    "   - Kỹ thuật này ít phức tạp hơn so với Selenium và dễ sử dụng hơn cho các trang web không yêu cầu tương tác động.\n",
    "\n",
    "### 3. Sử Dụng Pandas và Lưu Trữ Dữ Liệu:\n",
    "\n",
    "- **Tất Cả Các Scripts và Jupyter Notebooks:**\n",
    "   - Tất cả các script và notebooks đều sử dụng thư viện Pandas để xử lý dữ liệu sau khi thu thập.\n",
    "   - Dữ liệu được chuyển đổi thành DataFrame để dễ dàng thao tác và phân tích.\n",
    "   - Hỗ trợ lưu trữ dữ liệu vào tệp CSV, thuận tiện cho việc lưu trữ và phân tích dữ liệu.\n",
    "\n",
    "### So Sánh Tổng Quát:\n",
    "- **Sử Dụng Selenium:**\n",
    "   - Phù hợp với việc thu thập dữ liệu từ các trang web động và cần tương tác người dùng.\n",
    "   - Cần kỹ năng lập trình cao hơn và hiểu biết về trình duyệt web.\n",
    "\n",
    "- **Sử Dụng BeautifulSoup:**\n",
    "   - Đơn giản hơn và dễ sử dụng cho việc thu thập dữ liệu từ trang web tĩnh.\n",
    "   - Không cần tương tác động với trang web.\n",
    "\n",
    "- **Sử Dụng Pandas:**\n",
    "   - Là phần chung trong cả hai phương pháp, dùng để xử lý và lưu trữ dữ liệu một cách hiệu quả.\n",
    "\n",
    "Tùy thuộc vào yêu cầu cụ thể của nhiệm vụ thu thập dữ liệu, bạn có thể chọn Selenium cho các trang web đòi hỏi tương tác động hoặc BeautifulSoup cho các trang web tĩnh và dễ phân tích cú pháp. Pandas là công cụ xử lý dữ liệu phổ biến cho cả hai trường hợp."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bạn quan sát đúng. Phương pháp sử dụng BeautifulSoup trong các Jupyter Notebooks (`crawl_gold_price.ipynb`, `crawl_MSCI_EM_EFT.ipynb`, `crawl_NLR_EFT.ipynb`) có thể phụ thuộc vào việc sử dụng các tệp HTML đã được tải xuống và lưu trữ dưới dạng `.txt` hoặc các định dạng tương tự. Hãy xem xét từng phương pháp:\n",
    "\n",
    "### Phân Tích Cú Pháp HTML với BeautifulSoup:\n",
    "- Trong phương pháp này, dữ liệu thường được trích xuất từ các tệp HTML đã được tải xuống trước đó.\n",
    "- Các tệp như `gold_html.txt` chứa mã HTML của trang web mục tiêu, được tải xuống và lưu trữ trước.\n",
    "- Khi sử dụng BeautifulSoup, mã trong Jupyter Notebook sẽ đọc nội dung của các tệp này để phân tích cú pháp và trích xuất dữ liệu.\n",
    "- Điều này nghĩa là bạn cần truy cập trang web một lần, lưu mã HTML, sau đó sử dụng nó cho việc phân tích.\n",
    "\n",
    "### Sử Dụng Selenium:\n",
    "- Phương pháp này thường tự động và tương tác trực tiếp với trang web thông qua trình duyệt web.\n",
    "- Selenium tự động mở trình duyệt, điều hướng đến trang web, và thực hiện các hành động như nhấp chuột, nhập liệu vào form - tất cả đều được tự động hóa thông qua script.\n",
    "- Không yêu cầu tải xuống và lưu trữ HTML trước, Selenium hoạt động trực tiếp với phiên bản trực tuyến của trang web.\n",
    "\n",
    "### So Sánh:\n",
    "- **BeautifulSoup & Tệp HTML Đã Tải Xuống:**\n",
    "   - Cần bước tải xuống và lưu trữ HTML trước.\n",
    "   - Hữu ích khi làm việc với dữ liệu tĩnh hoặc khi trang web khó tương tác tự động.\n",
    "\n",
    "- **Selenium:**\n",
    "   - Tương tác trực tiếp và tự động với trang web.\n",
    "   - Phù hợp với trang web động, cần tương tác người dùng.\n",
    "\n",
    "Lựa chọn phương pháp phụ thuộc vào nhu cầu cụ thể của dự án và tính chất của trang web bạn muốn thu thập dữ liệu từ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My task: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- S&P 500: https://finance.yahoo.com/quote/%5EGSPC?p=^GSPC&.tsrc=fin-srch (S&P 500 (^GSPC)\n",
    "SNP - SNP Real Time Price. Currency in USD)\n",
    "- Russel 2000 Index: https://finance.yahoo.com/quote/%5ERUT?p=^RUT&.tsrc=fin-srch (Russell 2000 (^RUT)\n",
    "Chicago Options - Chicago Options Delayed Price. Currency in USD)\n",
    "- Dollar Index: https://finance.yahoo.com/quote/DX-Y.NYB/history   (ICE US Dollar Index - Index - C (DX-Y.NYB)\n",
    "ICE Futures - ICE Futures Real Time Price. Currency in USD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Giới Thiệu về Các Chỉ Số: S&P 500, Russel 2000 Index, Dollar Index\n",
    "\n",
    "1. **S&P 500:** https://finance.yahoo.com/quote/%5EGSPC?p=^GSPC&.tsrc=fin-srch (S&P 500 (^GSPC)\n",
    "SNP - SNP Real Time Price. Currency in USD)\n",
    "   - Chỉ số S&P 500 là một chỉ số chứng khoán của 500 công ty lớn niêm yết trên sàn giao dịch chứng khoán ở Mỹ.\n",
    "   - Nó thường được coi là chỉ số tốt nhất về hiệu suất của thị trường chứng khoán Mỹ và kinh tế rộng lớn hơn.\n",
    "\n",
    "2. **Russell 2000 Index:** https://finance.yahoo.com/quote/%5ERUT?p=^RUT&.tsrc=fin-srch (Russell 2000 (^RUT)\n",
    "Chicago Options - Chicago Options Delayed Price. Currency in USD)\n",
    "   - Russell 2000 là một chỉ số chứng khoán đại diện cho 2000 công ty nhỏ có vốn hóa thị trường thấp nhất trong tổng số 3000 công ty lớn nhất tại Mỹ, được theo dõi bởi Russell 3000 Index.\n",
    "   - Nó cung cấp một hình ảnh rõ ràng về hiệu suất của phân khúc công ty nhỏ và vừa.\n",
    "\n",
    "3. **Dollar Index (DXY):** https://finance.yahoo.com/quote/DX-Y.NYB/history   (ICE US Dollar Index - Index - C (DX-Y.NYB)\n",
    "ICE Futures - ICE Futures Real Time Price. Currency in USD)\n",
    "   - Dollar Index là một chỉ số đo lường giá trị của đồng đô la Mỹ so với một rổ tiền tệ của các đối tác thương mại chính của Mỹ.\n",
    "   - Nó thường được sử dụng để đánh giá sức mạnh tổng thể của đồng đô la Mỹ trên thị trường toàn cầu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "### Thu Thập Dữ Liệu:\n",
    "\n",
    "Cả **Trading Economics** và **Yahoo Finance** đều là nguồn tuyệt vời để thu thập dữ liệu về các chỉ số tài chính này. Việc lựa chọn giữa hai trang này phụ thuộc vào yếu tố như dễ dàng truy cập dữ liệu, giao diện người dùng, và sự phong phú của dữ liệu.\n",
    "\n",
    "### Sử Dụng HTML với BeautifulSoup hay Selenium?\n",
    "\n",
    "Lựa chọn giữa BeautifulSoup và Selenium phụ thuộc vào cách thức cung cấp dữ liệu trên trang web mà bạn chọn:\n",
    "\n",
    "- **Sử Dụng BeautifulSoup:**\n",
    "   - Nếu dữ liệu trên trang web là tĩnh và có thể truy cập trực tiếp thông qua HTML mà không cần tương tác động, bạn có thể sử dụng BeautifulSoup.\n",
    "   - Nó đơn giản hơn và ít tốn kém tài nguyên hơn so với Selenium.\n",
    "\n",
    "- **Sử Dụng Selenium:**\n",
    "   - Nếu trang web yêu cầu tương tác động như điều hướng qua các trang, chọn ngày, hoặc vượt qua các biện pháp bảo vệ, Selenium sẽ là lựa chọn tốt hơn.\n",
    "   - Selenium cung cấp khả năng tương tác mô phỏng người dùng thực tế với trình duyệt web.\n",
    "\n",
    "Tóm lại, lựa chọn công cụ phụ thuộc vào cấu trúc và độ phức tạp của trang web bạn muốn thu thập dữ liệu từ. Trong một số trường hợp, có thể cần phải kết hợp cả hai phương pháp để đạt hiệu quả cao nhất."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mỹ Linh: https://tradingeconomics.com/crypto  - beutiful soup \n",
    "Long https://finance.yahoo.com/quote/CL%3DF/history/ - selenium\n",
    "Minh Cường Marketwatch - selenium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chỉ số S&P 500, thường được biểu thị bằng ký hiệu `^GSPC` trên các sàn giao dịch, là một trong những chỉ số chứng khoán quan trọng và được theo dõi nhiều nhất tại Hoa Kỳ. Dưới đây là một số thông tin chính về chỉ số này:\n",
    "\n",
    "1. **Khái Niệm Cơ Bản:**\n",
    "   - S&P 500 là một chỉ số vốn hóa thị trường có trọng số, bao gồm 500 công ty lớn và đa dạng ngành nghề được niêm yết chủ yếu trên hai sàn giao dịch lớn nhất Hoa Kỳ là NYSE và NASDAQ.\n",
    "   - Chỉ số này được quản lý bởi Standard & Poor's, một công ty tài chính và dịch vụ thị trường Mỹ.\n",
    "\n",
    "2. **Đại Diện cho Thị Trường Chứng Khoán Mỹ:**\n",
    "   - S&P 500 được coi là một chỉ báo chính cho thị trường chứng khoán Mỹ, phản ánh tình trạng của thị trường và kinh tế Mỹ.\n",
    "   - Do bao gồm các công ty từ nhiều lĩnh vực kinh tế, nó cung cấp cái nhìn tổng quát về hiệu suất của doanh nghiệp lớn và thị trường chứng khoán.\n",
    "\n",
    "3. **Tính Toán Chỉ Số:**\n",
    "   - Chỉ số này được tính toán dựa trên vốn hóa thị trường điều chỉnh theo float của các công ty thành viên, nghĩa là chỉ tính phần vốn hóa của cổ phiếu được giao dịch tự do trên thị trường, không bao gồm cổ phiếu bị giữ kín.\n",
    "\n",
    "4. **Sử Dụng:**\n",
    "   - S&P 500 là một công cụ đầu tư quan trọng, thường được sử dụng như một chỉ số benchmark để đánh giá hiệu suất của quỹ đầu tư và các quản lý quỹ.\n",
    "   - Nó cũng là chỉ số được theo dõi bởi nhiều ETF và các sản phẩm đầu tư khác.\n",
    "\n",
    "### Kết Luận:\n",
    "S&P 500 (^GSPC) là một chỉ số rất quan trọng trong thị trường chứng khoán, phản ánh hiệu suất kinh tế rộng lớn và được sử dụng như một thước đo chuẩn mực cho các nhà đầu tư và quản lý quỹ tại Hoa Kỳ và trên toàn thế giới."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MSCI Inc. (MSCI)\n",
    "NYSE - NYSE Delayed Price. Currency in USD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dựa trên yêu cầu bạn đã mô tả, \"các feature khác chỉ lấy Price và tên cột cho nó sẽ là tên feature\" có thể được hiểu rằng cho mỗi chỉ số hoặc tài sản tài chính khác ngoại trừ Gold, bạn chỉ cần lấy dữ liệu từ cột \"Price\" trong dữ liệu nguồn của bạn. Tên cột này trong bảng dữ liệu cuối cùng sẽ được đặt theo tên của chỉ số hoặc tài sản đó. \n",
    "\n",
    "Về thuật ngữ \"Price\" ở đây, nó thường được hiểu là giá đóng cửa (Close) trong ngữ cảnh chứng khoán và tài chính, đặc biệt khi đề cập đến dữ liệu hàng ngày của các chỉ số chứng khoán hoặc ETF. Tuy nhiên, điều quan trọng là cần xác định rõ \"Price\" trong nguồn dữ liệu của bạn thực sự đại diện cho giá trị nào - có thể là giá đóng cửa, giá hiện tại, hoặc một giá trị khác tùy thuộc vào cách dữ liệu được trình bày.\n",
    "\n",
    "Để chắc chắn, bạn có thể cần xác minh rõ ràng từ người đưa ra yêu cầu hoặc kiểm tra cấu trúc và mô tả của dữ liệu nguồn mà bạn đang sử dụng. Điều này sẽ giúp đảm bảo bạn hiểu đúng và xử lý dữ liệu theo đúng yêu cầu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Đã xác thực - nó chính là gí close"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chỉ số S&P 500\n",
    "ký hiệu `^GSPC` trên các sàn giao dịch\n",
    "\n",
    "- 1 trong những chỉ số chứng khoán quan trọng và được theo dõi nhiều nhất tại Hoa Kỳ. Dưới đây là một số thông tin chính về chỉ số này:\n",
    "\n",
    "1. **Khái Niệm Cơ Bản:**\n",
    "   - S&P 500 là một chỉ số vốn hóa thị trường có trọng số, bao gồm 500 công ty lớn và đa dạng ngành nghề được niêm yết chủ yếu trên hai sàn giao dịch lớn nhất Hoa Kỳ là NYSE và NASDAQ.\n",
    "   - Chỉ số này được quản lý bởi Standard & Poor's, một công ty tài chính và dịch vụ thị trường Mỹ.\n",
    "\n",
    "2. **Đại Diện cho Thị Trường Chứng Khoán Mỹ:**\n",
    "   - S&P 500 được coi là một chỉ báo chính cho thị trường chứng khoán Mỹ, phản ánh tình trạng của thị trường và kinh tế Mỹ.\n",
    "   - Do bao gồm các công ty từ nhiều lĩnh vực kinh tế, nó cung cấp cái nhìn tổng quát về hiệu suất của doanh nghiệp lớn và thị trường chứng khoán.\n",
    "\n",
    "3. **Tính Toán Chỉ Số:**\n",
    "   - Chỉ số này được tính toán dựa trên vốn hóa thị trường điều chỉnh theo float của các công ty thành viên, nghĩa là chỉ tính phần vốn hóa của cổ phiếu được giao dịch tự do trên thị trường, không bao gồm cổ phiếu bị giữ kín.\n",
    "\n",
    "4. **Sử Dụng:**\n",
    "   - S&P 500 là một công cụ đầu tư quan trọng, thường được sử dụng như một chỉ số benchmark để đánh giá hiệu suất của quỹ đầu tư và các quản lý quỹ.\n",
    "   - Nó cũng là chỉ số được theo dõi bởi nhiều ETF và các sản phẩm đầu tư khác.\n",
    "\n",
    "### Kết Luận:\n",
    "S&P 500 (^GSPC) là một chỉ số rất quan trọng trong thị trường chứng khoán, phản ánh hiệu suất kinh tế rộng lớn và được sử dụng như một thước đo chuẩn mực cho các nhà đầu tư và quản lý quỹ tại Hoa Kỳ và trên toàn thế giới."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver     # import webdriver, for automating web browser \n",
    "from selenium.webdriver.edge.options import Options  # class Optims, for setting options web Edge \n",
    "from selenium.webdriver.common.by import By  # By class, for finding  elemnets on web \n",
    "import pandas as pd\n",
    "from datetime import datetime   # xử lý time \n",
    "import time                 # xử lý time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options=Options()     # tạo đối tượng Options để cấu hình trình duyệt. \n",
    "options.add_experimental_option('detach',True)    # Thêm tùy chọn 'detach' để cho web chạy ở chế độ tách rời (detach model)\n",
    "driver=webdriver.Edge(options=options)  # tạo đối tượng webdrive sử dụng trình duyệt Edge và truyền đối tượng options vào\n",
    "# driver.get('https://finance.yahoo.com/quote/CL%3DF/history/')\n",
    "driver.get('https://finance.yahoo.com/quote/NLR/history?p=NLR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2):  \n",
    "    # Chờ 5 giây để trang web tải xong\n",
    "    time.sleep(5)\n",
    "    \n",
    "    # Tìm và nhấp vào phần chọn ngày bắt đầu\n",
    "    setday=driver.find_element(By.XPATH,'//*[@id=\"Col1-1-HistoricalDataTable-Proxy\"]/section/div[1]/div[1]/div[1]/div/div/div[1]')\n",
    "    setday.click()\n",
    "    \n",
    "    time.sleep(3)\n",
    "    \n",
    "    # Nhập ngày bắt đầu là '01/11/2019' vào ô nhập\n",
    "    start_date=driver.find_element(By.XPATH,'//*[@id=\"dropdown-menu\"]/div/div[1]/input')\n",
    "    start_date.send_keys('11012019')\n",
    "    \n",
    "    # Nhập ngày kết thúc là '11/03/2023' vào ô nhập\n",
    "    end_date=driver.find_element(By.XPATH,'//*[@id=\"dropdown-menu\"]/div/div[2]/input')\n",
    "    end_date.send_keys('11032023')\n",
    "    \n",
    "    # Nhấp vào nút \"Áp dụng\" để xác nhận khoảng thời gian đã chọn\n",
    "    button=driver.find_element(By.XPATH,'//*[@id=\"dropdown-menu\"]/div/div[3]/button[1]')\n",
    "    button.click()\n",
    "    \n",
    "    time.sleep(3)\n",
    "    \n",
    "    # Nhấp vào nút \"Áp dụng\" để cập nhật dữ liệu hiển thị theo khoảng thời gian mới\n",
    "    apply=driver.find_element(By.XPATH,'//*[@id=\"Col1-1-HistoricalDataTable-Proxy\"]/section/div[1]/div[1]/button')\n",
    "    apply.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code gốc của Long\n",
    "# time.sleep(3)\n",
    "# while True:\n",
    "#     checker=driver.find_elements(By.XPATH,'//*[@id=\"Col1-1-HistoricalDataTable-Proxy\"]/section/div[2]/table/tbody/tr/td[1]')\n",
    "#     if len(checker)>=3484:\n",
    "#         break\n",
    "#     bottom=driver.find_element(By.XPATH,'//*[@id=\"Col1-1-HistoricalDataTable-Proxy\"]/section/div[2]/table/tfoot/tr')\n",
    "#     driver.execute_script(\"arguments[0].scrollIntoView(true);\", bottom)\n",
    "#     time.sleep(2)\n",
    "\n",
    "# Code chỉ down các phần data hiển thị (100 dòng), -> cần kéo xuống để data được load hết trên web trước khi thu thập\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"Col1-1-HistoricalDataTable-Proxy\"]/section/div[2]/table')))\n",
    "previous_length = 0\n",
    "while True:\n",
    "    # Cuộn xuống bảng\n",
    "    bottom = driver.find_element(By.XPATH, '//*[@id=\"Col1-1-HistoricalDataTable-Proxy\"]/section/div[2]/table/tfoot/tr')\n",
    "    driver.execute_script(\"arguments[0].scrollIntoView(true);\", bottom)\n",
    "\n",
    "    # Chờ cho trang web cập nhật\n",
    "    WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"Col1-1-HistoricalDataTable-Proxy\"]/section/div[2]/table/tbody/tr')))\n",
    "\n",
    "    # Kiểm tra xem số lượng hàng có thay đổi hay không\n",
    "    current_length = len(driver.find_elements(By.XPATH, '//*[@id=\"Col1-1-HistoricalDataTable-Proxy\"]/section/div[2]/table/tbody/tr'))\n",
    "    if current_length == previous_length:\n",
    "        break\n",
    "    else:\n",
    "        previous_length = current_length\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code gốc của Long: \n",
    "\n",
    "# date=driver.find_elements(By.XPATH,'//*[@id=\"Col1-1-HistoricalDataTable-Proxy\"]/section/div[2]/table/tbody/tr/td[1]')\n",
    "# # open=driver.find_elements(By.XPATH,'//*[@id=\"Col1-1-HistoricalDataTable-Proxy\"]/section/div[2]/table/tbody/tr/td[2]')\n",
    "# # high=driver.find_elements(By.XPATH,'//*[@id=\"Col1-1-HistoricalDataTable-Proxy\"]/section/div[2]/table/tbody/tr/td[3]')\n",
    "# # low=driver.find_elements(By.XPATH,'//*[@id=\"Col1-1-HistoricalDataTable-Proxy\"]/section/div[2]/table/tbody/tr/td[4]')\n",
    "# close=driver.find_elements(By.XPATH,'//*[@id=\"Col1-1-HistoricalDataTable-Proxy\"]/section/div[2]/table/tbody/tr/td[5]')\n",
    "\n",
    "# data_len=len(date)\n",
    "# save={\n",
    "#     'time':[],\n",
    "#     'close':[],\n",
    "# }\n",
    "# for day in range(data_len):\n",
    "#     numerical_form = datetime.strptime(date[day].text, '%b %d, %Y').strftime('%Y-%m-%d')\n",
    "#     save['time'].append(numerical_form)\n",
    "#     save['close'].append(close[day].text)\n",
    "# df=pd.DataFrame(save)\n",
    "\n",
    "# Bug: \n",
    "# ---> 15     save['close'].append(close[day].text)\n",
    "#      16 df=pd.DataFrame(save)\n",
    "# IndexError: list index out of range\n",
    "\n",
    "# Lỗi xảy ra khi truy cập một phần tử trong danh sách mà không tồn tại. \n",
    "# Có thể là do số lượng phần tử trong danh sách `close` không bằng số lượng phần tử trong danh sách `date`.\n",
    "# Điều này có thể xảy ra do dữ liệu không đồng nhất trên các hàng của bảng trên trang web.\n",
    "\n",
    "# Fix: Thay vì thu thập dữ liệu từng cột một -> xử lý từng hàng một.\n",
    "# => Đảm bảo dữ liệu thu thập từ mỗi hàng là đồng nhất và không có sự chênh lệch về số lượng giữa các cột. \n",
    "\n",
    "# Thu thập dữ liệu từng hàng một, \n",
    "data = []\n",
    "rows = driver.find_elements(By.XPATH, '//*[@id=\"Col1-1-HistoricalDataTable-Proxy\"]/section/div[2]/table/tbody/tr')\n",
    "for row in rows:\n",
    "    cells = row.find_elements(By.TAG_NAME, 'td')\n",
    "    if len(cells) >= 5:  # Đảm bảo hàng có ít nhất 5 cột (bao gồm cả cột \"Close\")\n",
    "        date_text = cells[0].text\n",
    "        close_text = cells[4].text  # Cột thứ 5 là \"Close\"\n",
    "        # Chuyển đổi định dạng ngày tháng\n",
    "        numerical_date = datetime.strptime(date_text, '%b %d, %Y').strftime('%Y-%m-%d')\n",
    "        data.append({'time': numerical_date, 'close': close_text})\n",
    "\n",
    "# Tạo DataFrame từ dữ liệu thu thập được\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Lưu vào CSV\n",
    "df.to_csv('crude_oil.csv', index=False)\n",
    "\n",
    "print(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Để tối ưu hóa code Selenium của bạn, chúng ta có thể tập trung vào một số khía cạnh sau:\n",
    "\n",
    "1. Giảm việc sử dụng time.sleep(): Thay vì sử dụng time.sleep(), hãy dùng WebDriverWait để chờ đợi đến khi các yếu tố cần thiết xuất hiện trên trang.\n",
    "\n",
    "2. Tối ưu hóa việc tìm kiếm phần tử trên trang:\n",
    "\n",
    "3. Sử dụng các XPaths hiệu quả hơn và tránh tìm kiếm các phần tử nhiều lần.\n",
    "Xử lý các ngoại lệ:\n",
    "\n",
    "4. Thêm xử lý ngoại lệ để bắt và xử lý các tình huống như không tìm thấy phần tử.\n",
    "Tối ưu hóa việc thu thập dữ liệu:\n",
    "\n",
    "Hạn chế việc truy cập nhiều lần đến cùng một phần tử trên trang.\n",
    "\n",
    "\n",
    "Để tối ưu hóa đoạn code Python sử dụng Selenium và Pandas của bạn, chúng ta có thể xem xét các yếu tố sau:\n",
    "\n",
    "Loại bỏ lặp lại mã: Tránh lặp lại mã không cần thiết bằng cách sử dụng hàm hoặc vòng lặp.\n",
    "Tối ưu hóa chờ đợi: Sử dụng chờ đợi thông minh để giảm thời gian không cần thiết.\n",
    "Cải thiện việc truy cập phần tử: Tối ưu hóa cách truy cập các phần tử trên trang.\n",
    "Sử dụng Pandas một cách hiệu quả: Tối ưu hóa cách thức xử lý dữ liệu bằng Pandas.\n",
    "Xử lý ngoại lệ và lỗi: Bảo đảm rằng mã có thể xử lý các trường hợp lỗi một cách linh hoạt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Để tối ưu hóa đoạn code của bạn, chúng ta có thể thực hiện các điều chỉnh sau:\n",
    "\n",
    "1. **Sử dụng Explicit Waits thay vì `time.sleep()`**: Thay vì sử dụng `time.sleep()`, hãy sử dụng `WebDriverWait` để chờ đợi cụ thể các phần tử trên trang. Điều này làm giảm thời gian chờ không cần thiết và tăng hiệu suất của code.\n",
    "\n",
    "2. **Tối ưu hóa việc tìm kiếm phần tử**: Định nghĩa một hàm để tái sử dụng mã khi tìm kiếm phần tử, giúp làm sạch và rõ ràng hơn.\n",
    "\n",
    "3. **Gộp các bước nhập liệu vào các trường**: Gộp các bước nhập liệu vào các trường nhập liệu để giảm lặp lại mã.\n",
    "\n",
    "Dưới đây là đoạn code tối ưu hóa:\n",
    "\n",
    "Trong phiên bản này, tôi đã thêm một số hàm giúp tái sử dụng mã và làm cho code rõ ràng hơn. Ngoài ra, tôi đã sử dụng `WebDriverWait` để chờ đợi các phần tử xuất hiện thay vì `time.sleep()`, giúp cải thiện hiệu suất."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Russell 2000 Index (thường được gọi là \"^RUT\" trên các nền tảng giao dịch) và \"Russell 2000 Index\" đề cập đến cùng một chỉ số chứng khoán. Đây là một chỉ số quan trọng trên thị trường chứng khoán Hoa Kỳ, phản ánh hiệu suất của khoảng 2000 công ty nhỏ và vừa, nằm trong phân khúc thấp nhất của Russell 3000 Index.\n",
    "\n",
    "Russell 2000 được xem là một chỉ báo quan trọng về tình hình kinh tế của phân khúc doanh nghiệp nhỏ tại Mỹ và thường được sử dụng bởi các nhà đầu tư để đánh giá và so sánh với các phân khúc thị trường khác. Chỉ số này được quản lý bởi FTSE Russell, một công ty con của London Stock Exchange Group.\n",
    "\n",
    "Khi bạn thấy \"Russell 2000 (^RUT) Chicago Options - Chicago Options Delayed Price. Currency in USD,\" điều này chỉ ra rằng thông tin giá cả được cung cấp có độ trễ (không phải thời gian thực) và được niêm yết tại sàn giao dịch tùy chọn ở Chicago, với giá trị tính bằng đô la Mỹ. Điều này không thay đổi bản chất của chỉ số, chỉ là cách thông tin về chỉ số được trình bày và giao dịch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Đóng gói thành hàm VÀ TỐI ƯU HÓA CODE \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- About Hàm `WebDriverWait` sử dụng các kỹ thuật chờ đợi trong Selenium để đảm bảo rằng các phần tử cần thiết trên trang web sẵn sàng cho việc thao tác trước khi tiến hành nhập liệu và thực hiện các hành động như click. Điều này giúp tránh các vấn đề liên quan đến việc thao tác các phần tử chưa sẵn sàng, điển hình trong các ứng dụng web hiện đại nơi nội dung thường được tải động.\n",
    "- Bạn cũng có thể sử dụng sleep(), cơ mà ko tối ưu = `WebDriverWait`\n",
    "\n",
    "```python\n",
    "from selenium.webdriver.support.ui import WebDriverWait  # Đợi cho một điều kiện nhất định trước khi thao tác tiếp theo\n",
    "from selenium.webdriver.support import expected_conditions as EC  # Các điều kiện dự kiến trong Selenium\n",
    "\n",
    "def set_date_and_apply(start_date, end_date):\n",
    "    \"\"\"Hàm để đặt ngày bắt đầu và kết thúc, sau đó áp dụng thay đổi.\"\"\"\n",
    "    # WebDriverWait(drive, seconds).until  # Đợi 1 điều kiện nhất định, trước khi thao tác tiếp theo (wait command - Selenium)\n",
    "    # Đảm bảo rằng một phần tử trên trang web có thể được nhấp vào (được định vị, được load xong),\n",
    "    ##  trước khi tiến hành thực hiện hành động click.\n",
    "\n",
    "    WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, '//*[@id=\"Col1-1-HistoricalDataTable-Proxy\"]/section/div[1]/div[1]/div[1]/div/div/div[1]'))).click()\n",
    "    # WebDriverWait(driver, 10): Chờ tối đa 10 giây trước khi xuất hiện lỗi timeout.\n",
    "    # EC.element_to_be_clickable((By.XPATH, '...')): Chờ đến khi phần tử được định vị bằng XPATH có thể nhấp vào.\n",
    "    # .click(): Thực hiện hành động click lên phần tử đó.    \n",
    "    WebDriverWait(driver, 10).until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"dropdown-menu\"]/div/div[1]/input'))).send_keys(start_date)\n",
    "    # EC.visibility_of_element_located((By.XPATH, '...')): Chờ đến khi phần tử XPATH xuất hiện trên trang và có thể nhìn thấy.\n",
    "    # .send_keys(...): Gửi phím được chỉ định vào phần tử đó.    \n",
    "    driver.find_element(By.XPATH, '//*[@id=\"dropdown-menu\"]/div/div[2]/input').send_keys(end_date)\n",
    "    driver.find_element(By.XPATH, '//*[@id=\"dropdown-menu\"]/div/div[3]/button[1]').click()\n",
    "    WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, '//*[@id=\"Col1-1-HistoricalDataTable-Proxy\"]/section/div[1]/div[1]/button'))).click()\n",
    "    #  chờ cho đến khi nút áp dụng có thể nhấp vào và sau đó thực hiện hành động click để áp dụng các thay đổi đã thiết lập với ngày bắt đầu và kết thúc.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So sánh 2 cái này \n",
    "```\n",
    "By.XPATH, '//*[@id=\"Col1-1-HistoricalDataTable-Proxy\"]/section/div[2]/table/tfoot/tr'\n",
    "\n",
    "By.XPATH, '//*[@id=\"Col1-1-HistoricalDataTable-Proxy\"]/section/div[2]/table/tbody/tr'\n",
    "```\n",
    "\n",
    "Cả hai biểu thức XPATH này được sử dụng để định vị các phần tử khác nhau trong cấu trúc HTML của một trang web, cụ thể là trên trang web Yahoo Finance. Đây là sự so sánh chi tiết giữa hai biểu thức này:\n",
    "\n",
    "1. **By.XPATH, `'//*[@id=\"Col1-1-HistoricalDataTable-Proxy\"]/section/div[2]/table/tfoot/tr'`**\n",
    "   - Định vị một hàng (`tr`) nằm trong phần `tfoot` (chân bảng) của một bảng (`table`).\n",
    "   - `tfoot` thường chứa thông tin tổng kết hoặc chú thích cho bảng và thường nằm ở cuối bảng.\n",
    "   - Trong ngữ cảnh của trang web Yahoo Finance, `tfoot` có thể chứa phần tử cuộn hoặc nút tải thêm dữ liệu, làm cho nó trở thành mục tiêu lý tưởng để cuộn xuống cuối cùng của bảng dữ liệu.\n",
    "\n",
    "2. **By.XPATH, `'//*[@id=\"Col1-1-HistoricalDataTable-Proxy\"]/section/div[2]/table/tbody/tr'`**\n",
    "   - Định vị tất cả các hàng (`tr`) nằm trong phần `tbody` (thân bảng) của cùng một bảng.\n",
    "   - `tbody` là phần chứa dữ liệu chính của bảng, bao gồm các hàng và cột với thông tin chi tiết.\n",
    "   - Trong trường hợp trang web Yahoo Finance, `tbody` chứa dữ liệu lịch sử về giá cổ phiếu hoặc các chỉ số tài chính khác, với mỗi hàng biểu diễn một ngày giao dịch cụ thể.\n",
    "\n",
    "**Sự Khác Biệt Chính:**\n",
    "- Phần `tfoot` được sử dụng để định vị phần cuối cùng của bảng, thường được sử dụng cho việc cuộn trang để tải thêm dữ liệu.\n",
    "- Phần `tbody` được sử dụng để thu thập dữ liệu thực tế từ các hàng của bảng, nơi chứa thông tin chi tiết mà người dùng muốn thu thập.\n",
    "\n",
    "Khi làm việc với web scraping, việc hiểu cách sử dụng các phần khác nhau của một bảng HTML là rất quan trọng để có thể chính xác xác định và thu thập dữ liệu bạn cần.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Đóng gói và optimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, '//*[@id=\"Col1-1-HistoricalDataTable-Proxy\"]/section/div[1]/div[1]/div[1]/div/div/div[1]'))).click()\n",
    "    # WebDriverWait(driver, 10): Chờ tối đa 10 giây trước khi xuất hiện lỗi timeout.\n",
    "    # EC.element_to_be_clickable((By.XPATH, '...')): Chờ đến khi phần tử được định vị bằng XPATH có thể nhấp vào.\n",
    "    # .click(): Thực hiện hành động click lên phần tử đó.    \n",
    "    WebDriverWait(driver, 10).until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"dropdown-menu\"]/div/div[1]/input'))).send_keys(start_date)\n",
    "    # EC.visibility_of_element_located((By.XPATH, '...')): Chờ đến khi phần tử XPATH xuất hiện trên trang và có thể nhìn thấy.\n",
    "    # .send_keys(...): Gửi phím được chỉ định vào phần tử đó.    \n",
    "    # driver.find_element(By.XPATH, '//*[@id=\"dropdown-menu\"]/div/div[2]/input').send_keys(end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         time  close\n",
      "0  02-11-2023  72.14\n",
      "1  01-11-2023  70.98\n"
     ]
    }
   ],
   "source": [
    "# Version 1: Optimize&Packing, but thường xuyên bị bug StaleElementReferenceException : \n",
    "\n",
    "from selenium import webdriver  # Thư viện để tự động điều khiển trình duyệt web\n",
    "from selenium.webdriver.edge.options import Options  # Các tùy chọn cho trình duyệt Edge\n",
    "from selenium.webdriver.common.by import By  # Các phương pháp tìm kiếm phần tử trong trang web\n",
    "from selenium.webdriver.support.ui import WebDriverWait  # Đợi cho một điều kiện nhất định trước khi thao tác tiếp theo\n",
    "from selenium.webdriver.support import expected_conditions as EC  # Các điều kiện dự kiến trong Selenium\n",
    "import pandas as pd  \n",
    "from datetime import datetime  # Thư viện để làm việc với thời gian và ngày tháng\n",
    "import time  # Thư viện để tạo độ trễ trong quá trình thực thi\n",
    "\n",
    "options=Options()     # tạo đối tượng Options, # các tùy chọn cho trình duyệt Edge \n",
    "options.add_experimental_option('detach',True)    # add option: 'detach' - web chạy ở chế độ tách rời (detach model)\n",
    "driver=webdriver.Edge(options=options)  # tạo đối tượng webdrive sử dụng trình duyệt Edge và truyền đối tượng options vào\n",
    "# driver.get('https://finance.yahoo.com/quote/CL%3DF/history/')\n",
    "driver.get('https://finance.yahoo.com/quote/NLR/history?p=NLR')\n",
    "\n",
    "def set_date_and_apply(start_date, end_date):\n",
    "    \"\"\"Hàm để đặt ngày bắt đầu và kết thúc, sau đó áp dụng thay đổi.\"\"\"\n",
    "    #### WebDriverWait(drive, seconds).until  # Đợi 1 điều kiện nhất định, trước khi thao tác tiếp theo (wait command - Selenium)\n",
    "    #### element_to_be_clickable: Đảm bảo rằng một phần tử trên trang web có thể được nhấp vào.\n",
    "    #### presence_of_element_located: Đảm bảo rằng một phần tử đã xuất hiện trên trang web.\n",
    "    #### visibility_of_element_located: Đảm bảo rằng một phần tử đã xuất hiện trên trang web và có thể nhìn thấy.\n",
    "    #### Trước khi tiến hành thực hiện hành động click.\n",
    "    #### Sử dụng WebDriverWait thay thế cho drive.find_element + time.sleep()\n",
    "\n",
    "    # time.sleep(5)\n",
    "    # driver.find_element(By.XPATH,'//*[@id=\"Col1-1-HistoricalDataTable-Proxy\"]/section/div[1]/div[1]/div[1]/div/div/div[1]').click()\n",
    "    WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH,'//*[@id=\"Col1-1-HistoricalDataTable-Proxy\"]/section/div[1]/div[1]/div[1]/div/div/div[1]'))).click()\n",
    "    #### WebDriverWait(driver, 10): Chờ tối đa 10 giây trước khi xuất hiện lỗi timeout.\n",
    "    #### EC.element_to_be_clickable((By.XPATH, '...')): Chờ đến khi phần tử được định vị bằng XPATH có thể nhấp vào.\n",
    "    #### .click(): Thực hiện hành động click lên phần tử đó.    \n",
    "    \n",
    "    driver.find_element(By.XPATH, '//*[@id=\"dropdown-menu\"]/div/div[1]/input').send_keys(start_date)\n",
    "    # time.sleep(3)  \n",
    "    # WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, '//*[@id=\"dropdown-menu\"]/div/div[1]/input'))).send_keys(start_date)\n",
    "    #### EC.visibility_of_element_located((By.XPATH, '...')): Chờ đến khi phần tử XPATH xuất hiện trên trang và có thể nhìn thấy.\n",
    "    #### .send_keys(...): Gửi phím được chỉ định vào phần tử đó.    \n",
    "    driver.find_element(By.XPATH,'//*[@id=\"dropdown-menu\"]/div/div[2]/input').send_keys(end_date)\n",
    "    # WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, '//*[@id=\"dropdown-menu\"]/div/div[2]/input'))).send_keys(end_date)\n",
    "    driver.find_element(By.XPATH,'//*[@id=\"dropdown-menu\"]/div/div[3]/button[1]').click()\n",
    "    # WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, '//*[@id=\"dropdown-menu\"]/div/div[3]/button[1]'))).click()\n",
    "    \n",
    "    # time.sleep(3)\n",
    "    driver.find_element(By.XPATH,'//*[@id=\"Col1-1-HistoricalDataTable-Proxy\"]/section/div[1]/div[1]/button').click()\n",
    "    # WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, '//*[@id=\"Col1-1-HistoricalDataTable-Proxy\"]/section/div[1]/div[1]/button'))).click()\n",
    "    ####  Chờ cho đến khi nút áp dụng có thể nhấp vào -> sau đó click.\n",
    "# for _ in range(2):  \n",
    "#     set_date_and_apply('11012023', '11032023')\n",
    "##### When set 2 vòng for như code cũ dùng drive và time.sleep()\n",
    "#####  thì bị BUG CỰC NHIỀU: StaleElementReferenceException, tương tác với phần tử web \n",
    "##### nhưng phần tử đó không còn tồn tại hoặc ko còn được truy cập, đã thay đổi          \n",
    "##### set_date_and_apply('11012023', '11032023')\n",
    "\n",
    "#### Đoạn nào cần chờ thì dùng WebDriverWait, \n",
    "####  đoạn nào không cần chờ thì dùng driver.find_element() cho đỡ bug. \n",
    "#### Cần chờ an toàn thì dùng time.sleep(), thời gian chờ tối ưu để load web\n",
    "\n",
    "set_date_and_apply('11012023', '11032023')  # TH ít data\n",
    "# set_date_and_apply('06012023', '11032023')     # TH nhiều data xem cuộn trang\n",
    "\n",
    "# Cuộn xuống để tải toàn bộ dữ liệu, (trường hợp chưa cuộn xuống, thì data chỉ tải tối đa 100 dòng)\n",
    "# Chờ cho trang web cập nhật  - có thể ko cần thiết khi mạng khỏe, web khỏe, nhưng để an toàn thì cần \n",
    "WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"Col1-1-HistoricalDataTable-Proxy\"]/section/div[2]/table')))\n",
    "previous_length = 0\n",
    "while True:\n",
    "    # Cuộn xuống bảng\n",
    "    bottom = driver.find_element(By.XPATH, '//*[@id=\"Col1-1-HistoricalDataTable-Proxy\"]/section/div[2]/table/tfoot/tr')\n",
    "    driver.execute_script(\"arguments[0].scrollIntoView(true);\", bottom)\n",
    "\n",
    "    # Chờ cho trang web cập nhật  - có thể ko cần thiết khi mạng khỏe, web khỏe, nhưng để an toàn thì cần \n",
    "    WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"Col1-1-HistoricalDataTable-Proxy\"]/section/div[2]/table/tbody/tr')))\n",
    "\n",
    "    # Kiểm tra xem số lượng hàng có thay đổi hay không\n",
    "    current_length = len(driver.find_elements(By.XPATH, '//*[@id=\"Col1-1-HistoricalDataTable-Proxy\"]/section/div[2]/table/tbody/tr'))\n",
    "    if current_length == previous_length:\n",
    "        break\n",
    "    else:\n",
    "        previous_length = current_length\n",
    "rows = driver.find_elements(By.XPATH, '//*[@id=\"Col1-1-HistoricalDataTable-Proxy\"]/section/div[2]/table/tbody/tr')\n",
    "\n",
    "# Thu thập dữ liệu theo hàng, (ban đầu theo cột bị bug: index out of the range)\n",
    "data = []   # list lưu dữ liệu\n",
    "for row in rows:\n",
    "    cells = row.find_elements(By.TAG_NAME, 'td')     \n",
    "    # find_elements, tìm tất cả các ô (các thẻ td trong HTML). find_elements(By.TAG_NAME, ...), find_elements(By.XPATH, ...)\n",
    "    if len(cells) >= 5:\n",
    "        date_text = cells[0].text\n",
    "        close_text = cells[4].text    # close ở cột cells[4]\n",
    "        numerical_date = datetime.strptime(date_text, '%b %d, %Y').strftime('%d-%m-%Y')\n",
    "        # datetime - Chuyển đổi dạng chuỗi STRP sang dạng STRF ngày tháng năm cụ thể %Y-%m-%d, hoặc %d-%m-%Y, ... (ví dụ từ 'Jan 01, 2020' sang '2020-01-01').\n",
    "        data.append({'time': numerical_date, 'close': close_text})\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv('crude_oil.csv', index=False)\n",
    "print(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lỗi `StaleElementReferenceException` xảy ra khi có sự thay đổi trong cấu trúc của trang web sau khi bạn đã lấy tham chiếu đến một phần tử. (thường gặp khi dùng WebDriverWait)\n",
    "=> Giải pháp:\n",
    "- Đoạn nào cần chờ (muốn tốc độ cao) thì dùng WebDriverWait, kết hợp driver.find_element()\n",
    "- Đoạn nào cần chờ an toàn (tốc độ chậm) thì dùng driver.find_element() + time.sleep()\n",
    "\n",
    "Thực nghiệm: code trên cho thấy tốc độ quét nhanh hơn nhiều so với Long khi Long cho time.sleep(5), time.sleep(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Đóng gói \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version 1: Optimize&Packing, but thường xuyên bị bug StaleElementReferenceException : \n",
    "\n",
    "from selenium import webdriver  # Thư viện để tự động điều khiển trình duyệt web\n",
    "from selenium.webdriver.edge.options import Options  # Các tùy chọn cho trình duyệt Edge\n",
    "from selenium.webdriver.common.by import By  # Các phương pháp tìm kiếm phần tử trong trang web\n",
    "from selenium.webdriver.support.ui import WebDriverWait  # Đợi cho một điều kiện nhất định trước khi thao tác tiếp theo\n",
    "from selenium.webdriver.support import expected_conditions as EC  # Các điều kiện dự kiến trong Selenium\n",
    "import pandas as pd  \n",
    "from datetime import datetime  # Thư viện để làm việc với thời gian và ngày tháng\n",
    "import time  # Thư viện để tạo độ trễ trong quá trình thực thi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set url, output_name, start_date, end_date\n",
    "\n",
    "def get_data(url, output_csv, start_date_input, end_date_input):\n",
    "\n",
    "    options=Options()     # tạo đối tượng Options, # các tùy chọn cho trình duyệt Edge \n",
    "    options.add_experimental_option('detach',True)    # add option: 'detach' - web chạy ở chế độ tách rời (detach model)\n",
    "    driver=webdriver.Edge(options=options)  # tạo đối tượng webdrive sử dụng trình duyệt Edge và truyền đối tượng options vào\n",
    "    # driver.get('https://finance.yahoo.com/quote/NLR/history?p=NLR')\n",
    "    driver.get(url)\n",
    "    \n",
    "    def set_date_and_apply(start_date, end_date):\n",
    "        \"\"\"Hàm để đặt ngày bắt đầu và kết thúc, sau đó áp dụng thay đổi.\"\"\"\n",
    "        #### WebDriverWait(drive, seconds).until  # Đợi 1 điều kiện nhất định, trước khi thao tác tiếp theo (wait command - Selenium)\n",
    "        #### element_to_be_clickable: Đảm bảo rằng một phần tử trên trang web có thể được nhấp vào.\n",
    "        #### presence_of_element_located: Đảm bảo rằng một phần tử đã xuất hiện trên trang web.\n",
    "        #### visibility_of_element_located: Đảm bảo rằng một phần tử đã xuất hiện trên trang web và có thể nhìn thấy.\n",
    "        #### Trước khi tiến hành thực hiện hành động click.\n",
    "        #### Sử dụng WebDriverWait thay thế cho drive.find_element + time.sleep()\n",
    "\n",
    "        # time.sleep(5)\n",
    "        # driver.find_element(By.XPATH,'//*[@id=\"Col1-1-HistoricalDataTable-Proxy\"]/section/div[1]/div[1]/div[1]/div/div/div[1]').click()\n",
    "        WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH,'//*[@id=\"Col1-1-HistoricalDataTable-Proxy\"]/section/div[1]/div[1]/div[1]/div/div/div[1]'))).click()\n",
    "        #### WebDriverWait(driver, 10): Chờ tối đa 10 giây trước khi xuất hiện lỗi timeout.\n",
    "        #### EC.element_to_be_clickable((By.XPATH, '...')): Chờ đến khi phần tử được định vị bằng XPATH có thể nhấp vào.\n",
    "        #### .click(): Thực hiện hành động click lên phần tử đó.    \n",
    "        \n",
    "        driver.find_element(By.XPATH, '//*[@id=\"dropdown-menu\"]/div/div[1]/input').send_keys(start_date)\n",
    "        # time.sleep(3)  \n",
    "        # WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, '//*[@id=\"dropdown-menu\"]/div/div[1]/input'))).send_keys(start_date)\n",
    "        #### EC.visibility_of_element_located((By.XPATH, '...')): Chờ đến khi phần tử XPATH xuất hiện trên trang và có thể nhìn thấy.\n",
    "        #### .send_keys(...): Gửi phím được chỉ định vào phần tử đó.    \n",
    "        driver.find_element(By.XPATH,'//*[@id=\"dropdown-menu\"]/div/div[2]/input').send_keys(end_date)\n",
    "        # WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, '//*[@id=\"dropdown-menu\"]/div/div[2]/input'))).send_keys(end_date)\n",
    "        driver.find_element(By.XPATH,'//*[@id=\"dropdown-menu\"]/div/div[3]/button[1]').click()\n",
    "        # WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, '//*[@id=\"dropdown-menu\"]/div/div[3]/button[1]'))).click()\n",
    "        \n",
    "        # time.sleep(3)\n",
    "        driver.find_element(By.XPATH,'//*[@id=\"Col1-1-HistoricalDataTable-Proxy\"]/section/div[1]/div[1]/button').click()\n",
    "        # WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, '//*[@id=\"Col1-1-HistoricalDataTable-Proxy\"]/section/div[1]/div[1]/button'))).click()\n",
    "        ####  Chờ cho đến khi nút áp dụng có thể nhấp vào -> sau đó click.\n",
    "    # for _ in range(2):  \n",
    "    #     set_date_and_apply('11012023', '11032023')\n",
    "    ##### When set 2 vòng for như code cũ dùng drive và time.sleep()\n",
    "    #####  thì bị BUG CỰC NHIỀU: StaleElementReferenceException, tương tác với phần tử web \n",
    "    ##### nhưng phần tử đó không còn tồn tại hoặc ko còn được truy cập, đã thay đổi          \n",
    "    ##### set_date_and_apply('11012023', '11032023')\n",
    "\n",
    "    #### Đoạn nào cần chờ thì dùng WebDriverWait, \n",
    "    ####  đoạn nào không cần chờ thì dùng driver.find_element() cho đỡ bug. \n",
    "    #### Cần chờ an toàn thì dùng time.sleep(), thời gian chờ tối ưu để load web\n",
    "\n",
    "    # set_date_and_apply('11012023', '11032023')  # TH ít data\n",
    "    # set_date_and_apply('06012023', '11032023')     # TH nhiều data xem cuộn trang\n",
    "    set_date_and_apply(start_date_input, end_date_input)\n",
    "    # Cuộn xuống để tải toàn bộ dữ liệu, (trường hợp chưa cuộn xuống, thì data chỉ tải tối đa 100 dòng)\n",
    "    # Chờ cho trang web cập nhật  - có thể ko cần thiết khi mạng khỏe, web khỏe, nhưng để an toàn thì cần \n",
    "    WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"Col1-1-HistoricalDataTable-Proxy\"]/section/div[2]/table')))\n",
    "    previous_length = 0\n",
    "    while True:\n",
    "        # Cuộn xuống bảng\n",
    "        bottom = driver.find_element(By.XPATH, '//*[@id=\"Col1-1-HistoricalDataTable-Proxy\"]/section/div[2]/table/tfoot/tr')\n",
    "        driver.execute_script(\"arguments[0].scrollIntoView(true);\", bottom)\n",
    "\n",
    "        # Chờ cho trang web cập nhật  - có thể ko cần thiết khi mạng khỏe, web khỏe, nhưng để an toàn thì cần \n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"Col1-1-HistoricalDataTable-Proxy\"]/section/div[2]/table/tbody/tr')))\n",
    "\n",
    "        # Kiểm tra xem số lượng hàng có thay đổi hay không\n",
    "        current_length = len(driver.find_elements(By.XPATH, '//*[@id=\"Col1-1-HistoricalDataTable-Proxy\"]/section/div[2]/table/tbody/tr'))\n",
    "        if current_length == previous_length:\n",
    "            break\n",
    "        else:\n",
    "            previous_length = current_length\n",
    "    rows = driver.find_elements(By.XPATH, '//*[@id=\"Col1-1-HistoricalDataTable-Proxy\"]/section/div[2]/table/tbody/tr')\n",
    "\n",
    "    # Thu thập dữ liệu theo hàng, (ban đầu theo cột bị bug: index out of the range)\n",
    "    data = []   # list lưu dữ liệu\n",
    "    for row in rows:\n",
    "        cells = row.find_elements(By.TAG_NAME, 'td')     \n",
    "        # find_elements, tìm tất cả các ô (các thẻ td trong HTML). find_elements(By.TAG_NAME, ...), find_elements(By.XPATH, ...)\n",
    "        if len(cells) >= 5:\n",
    "            date_text = cells[0].text\n",
    "            close_text = cells[4].text    # close ở cột cells[4]\n",
    "            numerical_date = datetime.strptime(date_text, '%b %d, %Y').strftime('%d/%m/%Y')\n",
    "            # datetime - Chuyển đổi dạng chuỗi STRP sang dạng STRF ngày tháng năm cụ thể %Y-%m-%d, hoặc %d-%m-%Y, ... (ví dụ từ 'Jan 01, 2020' sang '2020-01-01').\n",
    "            data.append({'time': numerical_date, 'close': close_text})\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    # df.to_csv('crude_oil.csv', index=False)\n",
    "    df.to_csv(output_csv, index=False)\n",
    "    print(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demo\n",
    "url = 'https://finance.yahoo.com/quote/NLR/history?p=NLR'\n",
    "output_csv = 'demo_url.csv'\n",
    "start_date_input = '11012023'     # 04/01/2023\n",
    "end_date_input = '11042023'       # 03/11/2023, set lên 04/11/2023 để lấy được ngày 03/11/2023\n",
    "get_data(url, output_csv, start_date_input, end_date_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crawl 3 chỉ số: Crawl Các Chỉ Số: S&P 500, Russel 2000 Index, Dollar Index\n",
    "- Mỗi chỉ số chỉ mất 6 phút để crawl 4000 dòng"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
